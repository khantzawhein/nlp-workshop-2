{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T17:35:14.887204Z",
     "start_time": "2024-12-14T17:35:13.561343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import emoji\n",
    "import csv\n",
    "\n",
    "from spacy.pipeline.lemmatizer import lemmatizer_score\n",
    "\n",
    "dataset = pd.read_csv(\"./IMDB Dataset.csv\")\n",
    "print(dataset.describe())\n",
    "\n",
    "file_to_write = open(\"./IMDB Dataset_cleaned.csv\", \"w\")\n",
    "new_csv = csv.writer(file_to_write)\n",
    "new_csv.writerow([\"review\", \"sentiment\"])\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "id": "4ab14446ca78c03e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T17:35:14.891893Z",
     "start_time": "2024-12-14T17:35:14.890029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter unwanted tokens, special chars, numbers, and extra spaces\n",
    "# also remove stop words, as well as converting to lowercase\n",
    "# it will also remove emoticon as it is a punctuation\n",
    "def clean_token(doc):\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if (not token.is_punct and not token.is_space\n",
    "                and not token.is_stop and not token.is_quote and token.is_ascii and not token.like_num):\n",
    "            # lemmatize the token\n",
    "            lemmatized_token = token.lemma_\n",
    "            cleaned_tokens.append(lemmatized_token.replace(\"--\", \"\").removesuffix(\"-\").removeprefix(\"-\").lower())\n",
    "    return cleaned_tokens"
   ],
   "id": "a69fc1c0ca81328d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T17:47:34.049662Z",
     "start_time": "2024-12-14T17:35:14.918256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text(text):\n",
    "    # Remove HTML like tags\n",
    "    regex_remove_html = re.compile(r'<.*?>')\n",
    "    text = regex_remove_html.sub('', text)\n",
    "    # convert unicode emojis to words\n",
    "    text = emoji.demojize(text)\n",
    "    doc = nlp(text)\n",
    "    cleaned_sentences = []\n",
    "    for sent in doc.sents:\n",
    "        cleaned_tokens = clean_token(sent)\n",
    "        cleaned_sentences.append(\" \".join(cleaned_tokens))\n",
    "    return \"\\n\".join(cleaned_sentences)\n",
    "\n",
    "\n",
    "for index, review in enumerate(dataset['review']):\n",
    "    processed_text = process_text(review)\n",
    "    new_csv.writerow([processed_text, dataset.get('sentiment')[index]])"
   ],
   "id": "671ab2e23f38a7b1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T17:47:34.064831Z",
     "start_time": "2024-12-14T17:47:34.063172Z"
    }
   },
   "cell_type": "code",
   "source": "file_to_write.close()",
   "id": "9f201e7559ed2ba1",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
